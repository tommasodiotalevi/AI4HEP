{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0rPcYkpCwCx",
    "tags": []
   },
   "source": [
    "# Data Quality Monitoring with Autoencoders at CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Credits:</b> A. Papanastassiou (INFN-Firenze), B. Camaiani (INFN-Firenze)<br>\n",
    "    Fifth ML-INFN Hachathon, <a href=https://agenda.infn.it/event/37650/overview> link </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi7DzzIuXCk6",
    "tags": []
   },
   "source": [
    "## LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Autoencoders are built using another type of layer, called LSTM layer. LSTM layers are capable of learning the complex dynamics within the temporal ordering of input sequences as well as use an internal memory to remember or use information across long input sequences. This is possible since LSTM is a type of Recurrent Neural Network (RNN) in which each neuron is built as multiple copies of the same unit:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/LSTM%20cell2.PNG\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "We can build two kinds of LSTM autoencoders, the first one is the Undercomplete one as before, where the structure of the layers is again showing a decrese followed by an increase of the number of nodes but with the complication that the output of each layer is duplicated to enter each of the copies of every node of the following layer (using return_sequences=True in the layer definition). For the latent layer, a RepeatVector layer is used to bring copies of the layer to the folowing decoding layer:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/LSTMAE.PNG\" width=\"650\"/>\n",
    "</div>\n",
    "\n",
    "The second kind is so-called \"Sparse\", where the encoding is performed using Dropout layer that randomly sets input units to 0 with a frequency of $rate$ at each step during training time. Inputs not set to 0 are scaled up by $1/(1 - rate)$ such that the sum over all inputs is unchanged. \\\n",
    "Both these models needs a reshaping of our input to allow each layer to see not one sample at a time but a certain window of them. We will first reashape the input, then rescale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIrF5KJCXB4g"
   },
   "outputs": [],
   "source": [
    "# This function creates the new reshaped input for the LSTM layer\n",
    "\n",
    "def reshape(X, time_steps=50):\n",
    "    X1 = []\n",
    "    for i in range(len(X) - time_steps-1):\n",
    "        t = X.loc[i:(i + time_steps-1)].values\n",
    "        X1.append(t)\n",
    "    return np.array(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the training dataset and apply the usual rescaling\n",
    "\n",
    "df_binsm_window=reshape(df_bins_train)\n",
    "x_train_w=np.array(df_binsm_window, dtype=np.float64)\n",
    "\n",
    "min_val = tf.reduce_min(x_train_w,axis=0)\n",
    "max_val = tf.reduce_max(x_train_w,axis=0)\n",
    "data_w = (x_train_w - min_val) / (max_val - min_val)\n",
    "data_w = np.where(np.isnan(data_w), 0, data_w)\n",
    "\n",
    "data_w=np.array(data_w, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKMiCI0_DETi",
    "outputId": "8162a12d-0a40-4bae-87ac-29c205935da5"
   },
   "outputs": [],
   "source": [
    "data_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue8vo-qNX3OR",
    "outputId": "fb475fa0-b00d-437e-e562-21b5134ed8d1"
   },
   "outputs": [],
   "source": [
    "# Define the sparse LSTM autoencoder \n",
    "\n",
    "autoencoder_LSTMs = keras.Sequential()\n",
    "autoencoder_LSTMs.add(keras.layers.LSTM(units=64, input_shape=(data_w.shape[1],data_w.shape[2]))) \n",
    "autoencoder_LSTMs.add(keras.layers.Dropout(rate=0.2)) #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "# one-dimensional encoded feature vector as output of the intermediate layer. A sample is encoded into a feature vector.\n",
    "autoencoder_LSTMs.add(keras.layers.RepeatVector(n=data_w.shape[1])) #Repeats the input n times (previous LSTM has no return sequence) .\n",
    "autoencoder_LSTMs.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "autoencoder_LSTMs.add(keras.layers.Dropout(rate=0.2))\n",
    "autoencoder_LSTMs.add(keras.layers.TimeDistributed(keras.layers.Dense(units=data_w.shape[2]))) #The TimeDistibuted layer takes the info from the previous layer and creates a vector with a length of the output layers.\n",
    "autoencoder_LSTMs.compile(loss='mae', optimizer='adam')\n",
    "autoencoder_LSTMs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k0z00EjvA662",
    "outputId": "a131cba2-4767-4bbe-bbb0-f2788164be46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "history=autoencoder_LSTMs.fit(data_w, data_w, epochs=100, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MAE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binsm_test_window=reshape(df_bins_test_3, time_steps=50)\n",
    "x_test_w=np.array(df_binsm_test_window, dtype=np.float64)\n",
    "\n",
    "min_val = tf.reduce_min(x_test_w,axis=0)\n",
    "max_val = tf.reduce_max(x_test_w,axis=0)\n",
    "data_test_w = (x_test_w - min_val) / (max_val - min_val)\n",
    "data_test_w = np.where(np.isnan(data_test_w), 0, data_test_w)\n",
    "\n",
    "data_test_w=np.array(data_test_w, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_w = autoencoder_LSTMs.predict(data_test_w)\n",
    "test_predictions_uw=test_predictions_w[:,0,:]\n",
    "test_data_uw=data_test_w[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_LSTMs = tf.math.reduce_mean(tf.math.abs(test_data_uw - test_predictions_uw), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_LSTMs)\n",
    "plt.xlabel('LS')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('MAE between input and output of the AE for testing data run3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_sparse=anomalies_finder(mae_LSTMs, 99.7, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sparse=np.sum(x_test_3, axis=0)\n",
    "\n",
    "cleaned_x_test_sparse =np.delete(x_test_3, [x-zeros for x in df_s.index[df_s[0].apply(lambda x: x in LS_sparse)].tolist()], axis=0)\n",
    "s_sparse=np.sum(cleaned_x_test_sparse, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot l and s and check if the anomalous LSs have disapeared\n",
    "plt.plot(l_sparse,ds = 'steps-mid',linewidth=1, label='uncleaned test run')\n",
    "plt.plot(s_sparse,ds = 'steps-mid',linewidth=1, label='cleaned test run')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel('METSig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data_uw[161,:],ds = 'steps-mid',linewidth=1, label='input')\n",
    "plt.plot(test_predictions_uw[161,:],ds = 'steps-mid',linewidth=1, label='output')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undercomplete LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_LSTMu = keras.Sequential()\n",
    "autoencoder_LSTMu.add(keras.layers.LSTM(units=64,input_shape=(data_w.shape[1],data_w.shape[2]),return_sequences=True)) #no return_sequence --> encoding\n",
    "autoencoder_LSTMu.add(keras.layers.LSTM(32, activation='relu', return_sequences=False))\n",
    "autoencoder_LSTMu.add(keras.layers.RepeatVector(n=data_w.shape[1]))\n",
    "autoencoder_LSTMu.add(keras.layers.LSTM(32, activation='relu', return_sequences=True))\n",
    "autoencoder_LSTMu.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\n",
    "autoencoder_LSTMu.add(keras.layers.TimeDistributed(keras.layers.Dense(units=data_w.shape[2])))\n",
    "autoencoder_LSTMu.compile(loss='mae', optimizer='adam')\n",
    "autoencoder_LSTMu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k0z00EjvA662",
    "outputId": "a131cba2-4767-4bbe-bbb0-f2788164be46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "history=autoencoder_LSTMu.fit(data_w, data_w, epochs=100, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MAE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_w = autoencoder_LSTMu.predict(data_test_w)\n",
    "test_predictions_uw=test_predictions_w[:,0,:]\n",
    "test_data_uw=data_test_w[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_LSTMu = tf.math.reduce_mean(tf.math.abs(test_data_uw - test_predictions_uw), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_LSTMu)\n",
    "plt.xlabel('LS')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('MAE between input and output of the AE for testing data run3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_5=anomalies_finder(mae_LSTMu, 99.7, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_under=np.sum(x_test_3, axis=0)\n",
    "\n",
    "cleaned_x_test_under =np.delete(x_test_3, [x-zeros for x in df_s.index[df_s[0].apply(lambda x: x in LS_5)].tolist()], axis=0)\n",
    "s_under=np.sum(cleaned_x_test_under, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot l and s and check if the anomalous LSs have disapeared\n",
    "plt.plot(l_under,ds = 'steps-mid',linewidth=1, label='uncleaned test run')\n",
    "plt.plot(s_under,ds = 'steps-mid',linewidth=1, label='cleaned test run')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel('METSig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Autoencoders appear to capture smaller anomalies more effectively than the Dense Undercomplete model in run3. Additionally, we have successfully detected empty or nearly empty LSs within the run that the other model failed to identify. While further optimization may enhance the performance of the dense model, overall, superior performance from LSTM Autoencoders is expected."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
