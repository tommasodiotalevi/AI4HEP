{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0rPcYkpCwCx",
    "tags": []
   },
   "source": [
    "# Data Quality Monitoring with Autoencoders at CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Credits:</b> A. Papanastassiou (INFN-Firenze), B. Camaiani (INFN-Firenze)<br>\n",
    "    Fifth ML-INFN Hachathon, <a href=https://agenda.infn.it/event/37650/overview> link </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvcQ8EPHCspJ",
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The CMS experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Compact Muon Solenoid (CMS) experiment  is one of the four main particle detectors located at the Large Hadron Collider (LHC) at CERN. It is  a multi-purpose experiment, designed to study high-energy proton-proton collisions to better understand the fundamental forces and particles that make up the universe.\n",
    "\n",
    "The CMS apparatus is composed of a complex system of sub-detectors to detect electrons, photons, muons and hadrons.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/sezione_cms.jfif\" width=\"400\" height=\"400\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only particles that CMS can not directly detect are neutrinos, because they interact very weakly with matter and for this they escape from the detection.\n",
    "To indirectly observe neutrinos, a kinematics observable called ***missing transverse energy*** (**MET**) is usually employed. MET is defined as:\n",
    "\n",
    "\n",
    "$$ \\text{MET} =  \\big|  - \\sum_i \\vec{p}_{T,i}\\big| $$\n",
    "\n",
    "where $\\vec{p}_{T,i}$ is the transverse momentum of the $i$-th reconstructed particle of the final state.\n",
    "Since the transverse momentum of the initial state is null, according to the law of conservation of momentum and energy, MET is expected to vanish if all products of a collision were detected. However, because neutrinos and other weakly interacting particles can escape the detector without being directly detected, their presence result in a non vanishing missing transverse energy value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particles that have a colour charge (like quarks and gluons) can not be directly observed as well. This is because a fundamental principle called *color confinement*, according to which colour charged particles can not be isolated and they always combine in ways that ensure their overall colour charge is color neutral. In order to obey colour confinement, quarks and gluons produced in strong interaction processes create other colored particles to form hadrons clustered in ***jets***, i.e. collimated *groups of colorless objects*.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/Sketch_PartonParticleCaloJet.png\" width=\"400\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LHC is a proton-proton collider, where protons are injected inside the accelerator: each time this happens it is called \"fill\". CMS operates during these fills and during each of them data is gathered in \"luminosity sections\", lumisections in short (LSs), that are sub-sections corresponding to around 23 seconds of data taking during which the instantaneous luminosity is almost constant. LSs are grouped in runs, of thousands of LSs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Quality Monitoring (DQM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being CMS composed of various subsystems, each serving a specific purpose in particle detection and measurement, issues in the different sub-detectors can arise due to various factors, such as radiation damage, electronic noise, and aging of components. The monitoring of data quality is therefore crucial both online, during the data taking, to promptly spot issues and act on them, and offline, to provide analysts with datasets that are cleaned against the occasional failures that may have crept in.\n",
    "\n",
    "We focus on the offline side, called Data Certification (DC): the final step of quality checks performed by DQM on recorded collision events. We look at integrated (over the whole run) quantities (\"Monitor Elements\") pertaining to hadronic jets and MET.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Elements (MEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different quantities related to MET and jets (collectively called JME) are monitored to accertain the quality of data. One of them is MET itself, another one is ***MET Significance*** defined as\n",
    "\n",
    "$$ \\text{METSig} =  \\frac{\\text{MET}}{\\sqrt{\\sum_i |\\vec{p}_{T,i}|}}\\,\\,\\, . $$\n",
    "\n",
    "Other quantities are associated with jets, for example the Jetmass. \n",
    "\n",
    "Anomalies can be visible in one or more of these MEs and usually are spotted by eye by operators instructed on histograms of \"healty\" or GOOD data. In the picture you can see an example of GOOD (green) and BAD (blue and orange) runs from the point of view of MET Significance.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/anomalous_runs.png\" width=\"400\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Per-Lumisection data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the specific case of quantities pertaining to hadronic jets and missing transverse momentum (MET), an issue in a few LSs would cause the entire run to be flagged as problematic (***BAD***), and thus removed from the pool of \"good-for-analysis\" data (***GOOD***).\n",
    "\n",
    "The possibility of accumulating quantities monitored for data quality purposes per-LS has been recently extended to JME MEs.\n",
    "This possibility allows for a higher granularity detection of anomalies, potentially enabling the saving of higher amounts of data from runs presenting only a limited set of anomalous LSs. Given the high number, ùëÇ(1000), of LSs to be analyzed for each run, an automated approach (rather than a manual one) for DC is required.  \n",
    "Machine Learning (ML), particularly Neural Networks (NN), can be implemented to this end.\n",
    "An unsupervised ML model based on a specific NN architecture called ***AutoEncoder*** (**AE**) is employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are a particular kind of unsupervised neural network capable of learning efficient codings of unlabelled data. The model is composed of an encoder and a decoder, the first one brings data to a representation of lower dimesionality (latent space), the second one brings it back to the original dimension.\n",
    "\n",
    "During training, the task is to **reconstruct the input approximately, preserving only the most relevant aspects of it**. The reconstruction loss quantifies the distance between input and output and must be minimized during training. What prohibits the Autoencoder to learn the Identity between input and output is its own architecture: a bottleneck for information provided e.g. by a decrease of the nodes number in the inner layers.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/Autoencoder_working0.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "For anomaly detection purpuses, the autoencoder is trained on non-anomalous data. If an anomaly is present in the testing set, it will result in a peak in the reconstruction loss.\n",
    "\n",
    "In the context of DQM, the Autoencoder is trained on ordered sequences of histograms related to a specific ME, each representing one LS, coming from a GOOD run. Then we will test the model on per-LS data coming from a run flagged BAD, meaning that some MEs, integrated over the run, show an uncommon shape. By looking at the reconstruction loss we will hopefully be able to tell if the anomaly affecting the run is restricted to a **limited set of LSs**. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://alpapana.web.cern.ch/Autoencoder_working_.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luminosity dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each run the istantaneous **luminosity decreases** and this brings to an appreciable evolution of the MEs observed per-lumisection. In particular a clear reduction in the magnitude of each quantity is visible. Being this is an unavoidable effect, we want our models to be able to spot anomalies occuring in every luminosity condition, therefore we would like to account for the luminosity decrease by integrating the luminosity information inside the model. There are many possible ways to do this but the easier one is to add the luminosity value of each LS to the input of the autoencoder both during training and testing. In this exercise we will not include this input because for the particular anomalies presented the model is powerful enough to complete the task without information about the luminosity of each LS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYMyBwB-DT8I",
    "tags": []
   },
   "source": [
    "### Load, read and understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka5dq2cBDafP"
   },
   "source": [
    "Let's start the hackathon by reading the dataset and understanding what we will work with. We are going to use sequences of histograms related to the Monitor Element METSig loaded from a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the data samples for this hands-on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.makedirs('data')\n",
    "    os.system(\"wget -q -O data/data.zip 'https://www.dropbox.com/scl/fo/ppvqy9lrz9be3zkgwtgmp/AKM_bLVCT99aMe58ME5dNp8?rlkey=ogogm7y1k7q3qb177tvofxz72&st=fkysjqlj&dl=0' && unzip data/data.zip -d data/ 2>/dev/null\")\n",
    "    os.remove(\"data/data.zip\")\n",
    "else:        \n",
    "    print(\"Data folder does exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the dataset containing the GOOD run to be used as training for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv='data/run_gggggg_METSig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJfOUm_7C8gD"
   },
   "outputs": [],
   "source": [
    "#Read the .csv file and convert it to pandas dataframe\n",
    "df=pd.read_csv(csv,names=range(2),sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "O80bFZiPCr1Z",
    "outputId": "de5cfa55-3a1d-4c28-b3d0-10441f7e136e"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains around a thousand and an half of entries: each one is composed by a LS number (first column) and the content of the corresponding histogram (second column).\\\n",
    "We are going to extract the sequence of histograms necessary to train the Autoencoder: first we order the dataset based on the lumisection number, then we extract the column containing the histograms and we unstack them based on the presence of the comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHxBt4woHyb0",
    "outputId": "32ff789d-67d0-4833-b1ef-d24b3cc89074"
   },
   "outputs": [],
   "source": [
    "df=df.drop([0])\n",
    "df[0] = df[0].astype(int)\n",
    "df_s=df.sort_values(by=[0],ignore_index=True)  # ordering by \"fromlumi\"\n",
    "df_bins=df_s[1].str.extractall('(\\d+)')[0].unstack()   # extract and unstack\n",
    "df_bins_train=df_bins.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want data for training to begin with non-zero values, so we are going to remove the first possibly empty lumisections. `zeros` would be the number of empty LSs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identiying empty LSs.\n",
    "\n",
    "x_train=np.array(df_bins, dtype=np.float64)  # convert df to numpy array\n",
    "zeros=0\n",
    "for i in range(x_train.shape[0]):\n",
    "    if np.all(x_train[i,:]==0):\n",
    "        zeros+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty LSs\n",
    "x_train_=x_train[zeros:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwOSgD-YIHx8"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the model with training data we want to make it more manageble via a rescaling in the [0,1] interval, this is a common practice for this kind of models. Different rescalings are possible, but one that we found very effecive is the following bin by bin rescaling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfgY6njOILKX"
   },
   "outputs": [],
   "source": [
    "min_val = np.min(x_train_,axis=0)\n",
    "max_val = np.max(x_train_,axis=0)\n",
    "data_= np.divide(x_train_, max_val - min_val, out=np.zeros_like(x_train_), where=max_val - min_val!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JooO_Y7JPepz",
    "outputId": "6367fe0f-e86c-4008-b747-90aaa04e63fb"
   },
   "outputs": [],
   "source": [
    "data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the entire training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the histogram of the whole run we sum over all the LSs. This is what usually is presented to people who perform data certification. In this case the ME is not showing any kind of anomalous shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=np.sum(x_train_, axis=0)\n",
    "plt.plot(l,ds = 'steps-mid',linewidth=1)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check luminosity dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the luminosity dependence in the run you can try the following script, it will plot the whole sequence of LSs histograms. Notice how the bin content decreases during the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "m = x_train.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line, = ax.plot(x_train[0, :], drawstyle='steps-mid', linewidth=1)\n",
    "ax.set_xlabel('METSIg')\n",
    "ax.set_title('LS=1')\n",
    "\n",
    "# Fix limits once so the canvas doesn't rescale each frame\n",
    "ax.set_xlim(0, x_train.shape[1] - 1)\n",
    "ymax = x_train.max()\n",
    "ax.set_ylim(0, ymax if ymax > 0 else 1)\n",
    "\n",
    "def update(i):\n",
    "    line.set_ydata(x_train[i, :])\n",
    "    ax.set_title(f'LS={i+1}')\n",
    "    return (line,)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=m, interval=50, blit=True)\n",
    "\n",
    "plt.close(fig)  # prevents duplicate static figure display\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a8rF5msS27C",
    "tags": []
   },
   "source": [
    "## Autoencoder model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to define our model. This will be a dense Autoencoder, meaning a neural network consisting of dense layers in which the encoding of information and subsequent decoding is obtained via a decrease (encoding) and following increase (decoding) of the number of nodes in the layers. This way the encoded representation inside the latent layer has a lower dimensionality and only the most relevant features can be preserved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dense Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\">\n",
    "<img src=\"https://alpapana.web.cern.ch/UnderDense.PNG\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice with the code and after the first layer add a 2 layers for encoding and the required number of layers for decoding the information. Last layer should have a specific shape. Activation functions should be `relu` for the hidden layers and `sigmoid` for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fy_60y1ISpRj"
   },
   "outputs": [],
   "source": [
    "training=data_\n",
    "input = keras.Input(shape=(training.shape[1],))\n",
    "learning_rate = 1e-7\n",
    "encoding_dim = 103\n",
    "encoding_dim_2 = 82\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',activity_regularizer=tf.keras.regularizers.l2(learning_rate))(input)\n",
    "latent = layers.Dense(encoding_dim_2, activation='relu')(encoded)\n",
    "decoded = layers.Dense(encoding_dim, activation='relu')(latent)\n",
    "decoded = layers.Dense(training.shape[1], activation='sigmoid')(decoded) # \"decoded\" is the lossy reconstruction of the input\n",
    "metrics = ['mse']\n",
    "model = keras.Model(input, decoded)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v08v54pTFtb",
    "tags": []
   },
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we define a batch size and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OoHVDMYTEiZ",
    "outputId": "4b808b6e-150a-44c4-9705-5feb0fb37c4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size =42\n",
    "epochs = 150\n",
    "history=model.fit(training, training, batch_size = batch_size, epochs = epochs,\n",
    "          validation_data=(training,training),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3-bCeqWT01T",
    "tags": []
   },
   "source": [
    "## Test the model on a BAD run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to repeat the operations performed earlier on the testing dataset, that is data coming from a run flagged BAD, for the same ME (METSig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_dataset(csv):\n",
    "    df=pd.read_csv(csv,names=range(15),sep=';')\n",
    "    df=df.drop([0])\n",
    "    df[0] = df[0].astype(int)\n",
    "    df_s=df.sort_values(by=[8],ignore_index=True)\n",
    "    df_bins=df_s[1].str.extractall('(\\d+)')[0].unstack()\n",
    "    df_bins_train=df_bins.reset_index(drop=True)\n",
    "    x_train=np.array(df_bins, dtype=np.float64)\n",
    "    zeros=0\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if np.all(x_train[i,:]==0):\n",
    "            zeros+=1\n",
    "        else:\n",
    "            break\n",
    "    x_train_=x_train[zeros:,:]\n",
    "    #x_train[155]=x_train[155]+(x_train[70,:]-x_train[71])//3\n",
    "    #x_train[155]=x_train[155]+np.array([  0.,   0.,   0.,   0.,   0.,   0.,   0., 1.,  7.,  17.,  13.,  27.,  39.,  57.,  63.,  73.,  80., 86., 115., 153., 210., 337., 426., 472., 404., 313., 211., 128., 73.,  47.,  22.,  14.,   4.,   2.,   0.,   0.,   0.,   0., 0., 0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 0.,   0.,   0.])\n",
    "    min_val = np.min(x_train_,axis=0)\n",
    "    max_val = np.max(x_train_,axis=0)\n",
    "    data_= np.divide(x_train_, max_val - min_val, out=np.zeros_like(x_train_), where=max_val - min_val!=0)\n",
    "    return zeros, data_, df_s, x_train, df_bins_train\n",
    "\n",
    "#zeros: the number of empty LSs\n",
    "#data_: the dataset rescaled (as np array)\n",
    "#df_s: the dataset before the extraction of the histograms (contains the LS number)\n",
    "#x_train: the dataset before rescaling (as np array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv='data/run_b1bbbb_METSig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros, testing, df_s, x_test, df_bins_test=build_test_dataset(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained model to check the run for anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39XocwM4T5mR",
    "outputId": "8e4049b5-048f-4641-fa0b-74e5b7da4658"
   },
   "outputs": [],
   "source": [
    "test_predictions=model.predict(testing)\n",
    "\n",
    "mse_dense = tf.math.reduce_mean(tf.math.pow(testing - test_predictions, 2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "b8sDGqSwVnTX",
    "outputId": "b52a311d-ede8-4c81-ecfe-0b909bc16f4c"
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_dense)\n",
    "plt.xlabel('LS')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE between input and output of the AE for testing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many anomalies you think are present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGuT2oG374jZ",
    "tags": []
   },
   "source": [
    "## Quantitative analysis of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the higher peaks by calculating the 99.x percentile of the reconstruction loss or of a moving average of the loss, as in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXfY4E4cynbx"
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=1) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def anomalies_finder(mse_dense, percentile, df_s, zeros, n):\n",
    "    data=moving_average(mse_dense, n)\n",
    "    #your code here...\n",
    "    d = np.percentile(data, percentile)\n",
    "    anomalies=np.where(mse_dense>d)[0]\n",
    "\n",
    "    LS=[]\n",
    "    for i in range(anomalies.shape[0]):\n",
    "        LS.append(df_s[0][anomalies[i]+zeros])\n",
    "    print(LS)\n",
    "    return LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS=anomalies_finder(mse_dense, 99.5, df_s, zeros, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many anomalies at 99.5 percentile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot the run with and without the identified anomalous LSs. This will help us verifying if these are significant anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRbTU4W7EN1y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sum over all the LSs of the non rescaled data (x_test)\n",
    "l=np.sum(x_test, axis=0)\n",
    "#subtract from x_test the anomalous LSs and sum over the remaining LSs\n",
    "cleaned_x_test =np.delete(x_test, [x-zeros for x in df_s.index[df_s[0].apply(lambda x: x in LS)].tolist()], axis=0)\n",
    "s=np.sum(cleaned_x_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "1KNoj0DnEWtz",
    "outputId": "708d3638-cad8-4318-8b4a-a75862b1fc1d"
   },
   "outputs": [],
   "source": [
    "#Plot l and s and check if the anomalous LSs have disapeared\n",
    "plt.plot(l,ds = 'steps-mid',linewidth=1, label='uncleaned test run')\n",
    "plt.plot(s,ds = 'steps-mid',linewidth=1, label='first cleaned test run')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel('METSig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_2=anomalies_finder(mse_dense, 99.9, df_s, zeros, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_x_test_2 =np.delete(x_test, [x-zeros for x in df_s.index[df_s[0].apply(lambda x: x in LS_2)].tolist()], axis=0)\n",
    "s_2=np.sum(cleaned_x_test_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot l and s and check if the anomalous LSs have disapeared\n",
    "plt.plot(l,ds = 'steps-mid',linewidth=1, label='uncleaned test run')\n",
    "plt.plot(s_2,ds = 'steps-mid',linewidth=1, label='second cleaned test run')\n",
    "plt.plot(s,ds = 'steps-mid',linewidth=1, label='first cleaned test run')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel('METSig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the whole run, you can spot the anomalies by eye ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = x_test.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line, = ax.plot(x_test[0, :], drawstyle='steps-mid', linewidth=1)\n",
    "ax.set_xlabel('METSIg')\n",
    "ax.set_title('LS=1')\n",
    "\n",
    "# Fix limits once so the canvas doesn't rescale each frame\n",
    "ax.set_xlim(0, x_test.shape[1] - 1)\n",
    "ymax = x_test.max()\n",
    "ax.set_ylim(0, ymax if ymax > 0 else 1)\n",
    "\n",
    "def update(i):\n",
    "    line.set_ydata(x_test[i, :])\n",
    "    ax.set_title(f'LS={i+1}')\n",
    "    return (line,)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=m, interval=50, blit=True)\n",
    "\n",
    "plt.close(fig)  # prevents duplicate static figure display\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Another BAD run to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained we can test it on another anomalous run. for the sake of clarity we will refer to it as run3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv='data/run_b2bbbb_METSig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_3, testing_3, df_s_3, x_test_3, df_bins_test_3=build_test_dataset(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros, testing, df_s, x_test, df_bins_test=build_test_dataset(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_3=model.predict(testing_3)\n",
    "\n",
    "mse_dense_3 = tf.math.reduce_mean(tf.math.pow(testing_3 - test_predictions_3, 2), axis = 1)\n",
    "mae_dense_3 = tf.math.reduce_mean(tf.math.abs(testing_3 - test_predictions_3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_dense_3)\n",
    "plt.xlabel('LS')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE between input and output of the AE for testing data run3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want to select the most relevant peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_3=anomalies_finder(mse_dense_3, 99.9, df_s_3, zeros_3, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_3=np.sum(x_test_3, axis=0)\n",
    "cleaned_x_test_3 =np.delete(x_test_3, [x-zeros_3 for x in df_s_3.index[df_s_3[0].apply(lambda x: x in LS_3)].tolist()], axis=0)\n",
    "s_3=np.sum(cleaned_x_test_3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot l and s and check if the anomalous LSs have disapeared\n",
    "plt.plot(l_3,ds = 'steps-mid',linewidth=1, label='uncleaned test run')\n",
    "plt.plot(s_3,ds = 'steps-mid',linewidth=1, label='cleaned test run')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel('METSig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is another anomaly the Autoencoder was not able to detect or it corresponds to one of the peaks but it is not high enough to be distinguished.\\\n",
    "One possible way out is to try modyfing the structure adding layers or changing hyperparameters. Do it as homework. You can also try to test with another function like `mae`. Is there an improvement? \\\n",
    "Another way is to try a different Autoencoder model..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
