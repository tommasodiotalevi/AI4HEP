{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pNN Tutorial\n",
    "The entire workflow of *defining*, *training*, and *evaluating* parametric neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up (only execute once, at the beginning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the datasets (HEPMASS-IMB + HEPMASS test-set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download HEPMASS-IMB archive from Zenodo; and save it under data\\\n",
    "!mkdir data\n",
    "!wget -O data/hepmass-imb.zip https://zenodo.org/record/6453048/files/hepmass-imb.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "!unzip data/hepmass-imb.zip -d data/hep-imb\n",
    "\n",
    "# delete archive\n",
    "!rm data/hepmass-imb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test-set of HEPMASS\n",
    "!wget -O data/all_test.csv.gz http://archive.ics.uci.edu/ml/machine-learning-databases/00347/all_test.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "!gzip -d data/all_test.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And prepare HEPMASS, before usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process:\n",
    "#  - specify \"-d\" if you want to delete \"all_test.csv\"\n",
    "!python3 process_csv.py data/all_test.csv data/hepmass/test.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following cell we force `TF` to use only the CPU, this is due the training sequences that require sampling numbers which is quite slow on the GPU.\n",
    "- The GPU may still be advantageous in case the pNN is convolutional. So in such case comment this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from script import utils, cms\n",
    "from script.utils import free_mem\n",
    "\n",
    "from script.models.layers import Divide\n",
    "from script.datasets import Hepmass, Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful when working on notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads the modified source files, automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nice looking plots, and fix the randomness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.plot.set_style()\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration\n",
    "In this section we load the `HEPMASS-IMB` dataset (you can find it on [Zenodo](https://doi.org/10.5281/zenodo.6453048)), which is a modification of `HEPMASS` ([UCI ML repository](http://archive.ics.uci.edu/ml/datasets/hepmass)).\n",
    "- The dataset has 28 features: 26 of them describe each event (entry), one is the class label, and the last *mass feature* represents the physics parameter (i.e. the signal mass hypothesis).\n",
    "- The event related features are already normalized, to have approximately zero-mean and unitary variance.\n",
    "- There are five mass points: $\\{500,750,1000,1250,1500\\}$ GeV.\n",
    "- There is one signal and one background process: the signal is labeled with class `1`, and the background `0`.\n",
    "- The training background samples are the same of `HEPMASS`, while the signal has been greatly reduce to make the dataset be more challenging and realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Benchmark()\n",
    "\n",
    "# the actual loading occurs here:\n",
    "data.load(signal='data/hep-imb/imbalanced_signal.csv',\n",
    "          bkg='data/hep-imb/imbalanced_background.csv', features=Hepmass.FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `.load()` function expects the dataset to be divided in two parts: a signal and background ones.\n",
    "* Both parts must be either a `csv` file (if so specify the path where it's stored), or a (list) of `pd.DataFrame` (assuming the data-frame have been already loaded into memory). In case you provide a list of data-frames, the function will concatenate each into one.\n",
    "* Each `csv` expects the following columns: `[type, mass, name, weight]`; the `name` column is optional for the signal csv.\n",
    "* Also you can specify which features to load, through the `features` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once loaded (or merged), we can inspect both the signal and background:\n",
    "data.signal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.background.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can access the signal and background data-frames, though the fields `.signal` and `.background` respectively.\n",
    "* The class label is stored in the `type` column; the mass feature under `mass`, the name of the background processes at `name` (in this case you see the default name: \"background\"): this column can be useful to retrieve the samples belonging to each bkg process, if more than one; lastly, the `weight` column can be used to weight the samples: in this case each entry has a $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays all the mass hypotheses\n",
    "data.signal['mass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training samples\n",
    "len(data.signal), len(data.background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 100\n",
    "df = data.ds  # .ds contains both signal and bkg dataframes\n",
    "mass = data.signal['mass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = utils.get_plot_axes(rows=5, cols=5, size=(12, 10))\n",
    "axes = np.reshape(axes, newshape=[-1])\n",
    "\n",
    "bkg = data.background\n",
    "sig = data.signal\n",
    "\n",
    "for i, col in enumerate(data.columns['feature'][1:-1]):\n",
    "    ax = axes[i]\n",
    "\n",
    "    stats = df[col].describe()\n",
    "    value_range = (stats['min'], stats['max'])\n",
    "\n",
    "    bkg[col].plot(kind='hist', bins=bins, histtype='step', label='bkg', hatch='//', ax=ax,\n",
    "                  range=value_range, linewidth=2, weights=np.ones_like(bkg[col]) / len(mass))\n",
    "\n",
    "    for m in mass:\n",
    "        sig[sig['mass'] == m][col].plot(kind='hist', bins=bins, histtype='step', ax=ax,\n",
    "                                        label=f'{int(m)} GeV', range=value_range, linewidth=2)\n",
    "\n",
    "    ax.set_ylabel('Weighted Num. Events')\n",
    "    ax.set_xlabel(col)\n",
    "\n",
    "    ax.legend(loc='best')\n",
    "    free_mem()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Definition\n",
    "In this section we're going to instantiate a pNN model as a feed-forward neural network (since our data is tabular.) In particular we define an *affine-pNN* with:\n",
    "* *pre-processing layers* applied on the input nodes;\n",
    "* 4 layers with $[300, 150, 100, 50]$ units, `ReLU` activation, and `Dropout`;\n",
    "* the *affine-conditioning* mechanism applied at each intermediate layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing layers (at `script.models.layers`):\n",
    "* `Divide(v)`: divides the input by the provided value `v`.\n",
    "* `Clip(v_min, v_max)`: bounds the input value between `v_min` and `v_max`.\n",
    "* `StandardScaler(mean, std)`: standardizes the input by subtracting by the provided `mean`, and then dividing by the provided standard deviation (`std`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case, HEPMASS has the features to be already standardized,\n",
    "# so we only normalize the mass feature \"m\" by dividing it by 1000.\n",
    "preproc = {'m': [Divide(1000.0)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pre-processing layers are defined by means of a `dict`.\n",
    "* The dict *keys* indicate on which input the layer(s) have to be applied: possible values are `\"x\"` for the features, and `\"m\"` for the mass (physics parameter).\n",
    "* The dict *values* should be a list of layers, each of them will be applied sequentially on the output of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a model we can either instantiate it (from `script.models.pnn` or `script.models.affine`) and then compile, or use the utility function `script.utils.get_compiled_pnn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, checkpoint = utils.get_compiled_pnn(data, units=[300, 150, 100, 50], activation=tf.nn.relu,\n",
    "                                           conditioning=dict(method='affine', place='all'),\n",
    "                                           dropout=0.25, kernel_initializer='he_uniform',\n",
    "                                           preprocess=preproc, lr=5e-4, save='tutorial/affine_pnn',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                           bias_regularizer=tf.keras.regularizers.l2(1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function will return a compiled `model` (with default `Adam` optimizer) ready to be trained.\n",
    "* If we specify the `save` argument (a string), the function will also create a `tf.keras.callbacks.ModelCheckpoint` instance (`checkpoint`) to save the model's weights during training.\n",
    "* The hyper-parameters can be directly passed as `kwargs`. In this case we have set the `units` (which also determines the number of hidden layers), optimizer's learning rate (`lr`), pre-processing layers (`preprocess`), the `activation` function, and finally the regularization (in terms of `dropout`, and l2 weight decay with `kernel_regularizer` and `bias_regularizer`.)\n",
    "\n",
    "In the hyper-params, we also specified the conditioning mechanism with the `conditioning` dict:\n",
    "* we can choose the **kind** (by `method`) which can be \"concat\" (concatenation-based), \"biasing\" (conditional biasing), \"scaling\" (conditional scaling), and \"affine\" (affine-conditioning).\n",
    "* and the **location** (where conditioning occurs) by specifying the `place` argument, that can be either `start` (right after pre-processed input layers), `all` (after each non-linearity), and `end` (just before the output layer.)\n",
    "\n",
    "This results in the following model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing -> (dense -> conditioning -> dropout) x4 -> output (sigmoid)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training\n",
    "Before training a pNN we pick a training `Sequence` that implements either or both: *parameter assignment* for the background data, and *balanced training*.\n",
    "\n",
    "Available sequences are:\n",
    "* `cms.data.IdenticalSequence`: implements the identical sampling strategy for mass assignment.\n",
    "* `cms.data.UniformSequence`: assigns the mass from a uniform distribution on the mass interval.\n",
    "* `cms.data.BalancedUniformSequence`: allows or not to sample the mass uniformly, additionally it balances the mini-batches. It supports *class* balance (`balance_signal=False` and `balance_bkg=False`), *signal* balance (`balance_signal=True` and `balance_bkg=False`), *background* balance (`balance_signal=False` and `balance_bkg=True`), and *full* balance (`balance_signal=True` and `balance_bkg=True`).\n",
    "* `cms.data.BalancedIdenticalSequence`: behaves like the `BalancedUniformSequence` but the mass is assigned following the identical strategy.\n",
    "\n",
    "These classes have a handy method (`get_data()`) that takes care of splitting the provided data into training and validation sets (which are `tf.data.Dataset` objects). Specific arguments to the sequence can be provided as `kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identical (sampled) sequence, without mini-batch balancing\n",
    "train, valid = cms.data.IdenticalSequence.get_data(data, train_batch=1024,\n",
    "                                                   features=data.columns['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train, epochs=25, validation_data=valid, verbose=2,\n",
    "                    # you can have other callbacks (e.g. early stop) as well\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "utils.plot_history(history, keys=['loss', 'binary_accuracy', 'auc', 'weight-norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation\n",
    "We evaluate our pNN on the test-set of `HEPMASS`, and measure various metrics such as the AUC of the ROC and PR (precision-recall) curves, as well as the significance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test-set\n",
    "test = Hepmass()\n",
    "test.load(path=Hepmass.TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best set of weights\n",
    "utils.load_from_checkpoint(model, path=model.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up models for comparison: in this case we only have one model.\n",
    "models = {'affine-pNN': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evaluation metrics on each mass point, per model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc, pr, ams = evaluation.compare_table(models, dataset=test, ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "evaluation.pivot(df=pd.DataFrame(roc), dataset=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision recall\n",
    "evaluation.pivot(df=pd.DataFrame(pr), dataset=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance ratio\n",
    "evaluation.pivot(df=pd.DataFrame(ams), dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.compare(models, dataset=test, legend='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
